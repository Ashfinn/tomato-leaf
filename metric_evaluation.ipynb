{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "848aa358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating fastvit_t8 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 501/501 [06:16<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for fastvit_t8:\n",
      "Accuracy: 0.9863\n",
      "Precision: 0.9864\n",
      "Recall: 0.9863\n",
      "F1-Score: 0.9863\n",
      "ROC-AUC: 0.9998\n",
      "\n",
      "Classification Report:\n",
      " {'Tomato_Bacterial_spot': {'precision': 0.9855880985588099, 'recall': 0.996708979783733, 'f1-score': 0.9911173445535297, 'support': 2127.0}, 'Tomato_Early_blight': {'precision': 0.9587525150905433, 'recall': 0.953, 'f1-score': 0.9558676028084253, 'support': 1000.0}, 'Tomato_Late_blight': {'precision': 0.9817423056859677, 'recall': 0.9858564693556836, 'f1-score': 0.9837950862519603, 'support': 1909.0}, 'Tomato_Leaf_Mold': {'precision': 0.9906347554630593, 'recall': 1.0, 'f1-score': 0.9952953476215368, 'support': 952.0}, 'Tomato_Septoria_leaf_spot': {'precision': 0.9665924276169265, 'recall': 0.9802371541501976, 'f1-score': 0.9733669750490609, 'support': 1771.0}, 'Tomato_Spider_mites_Two_spotted_spider_mite': {'precision': 0.9886972040452112, 'recall': 0.9916467780429594, 'f1-score': 0.9901697944593387, 'support': 1676.0}, 'Tomato__Target_Spot': {'precision': 0.9955588452997779, 'recall': 0.957977207977208, 'f1-score': 0.9764065335753176, 'support': 1404.0}, 'Tomato__Tomato_YellowLeaf__Curl_Virus': {'precision': 1.0, 'recall': 0.9928304239401496, 'f1-score': 0.9964023150320663, 'support': 3208.0}, 'Tomato__Tomato_mosaic_virus': {'precision': 0.9638242894056848, 'recall': 1.0, 'f1-score': 0.9815789473684211, 'support': 373.0}, 'Tomato_healthy': {'precision': 0.9974811083123426, 'recall': 0.9956002514142049, 'f1-score': 0.9965397923875432, 'support': 1591.0}, 'accuracy': 0.9863219036912123, 'macro avg': {'precision': 0.9828871549478324, 'recall': 0.9853857264664135, 'f1-score': 0.98405397391072, 'support': 16011.0}, 'weighted avg': {'precision': 0.9864145883982246, 'recall': 0.9863219036912123, 'f1-score': 0.9863140859665127, 'support': 16011.0}}\n",
      "\n",
      "=== Model Comparison ===\n",
      "            Accuracy  Precision    Recall  F1-Score   ROC-AUC\n",
      "fastvit_t8  0.986322   0.986415  0.986322  0.986314  0.999798\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add missing imports\n",
    "from torchvision import transforms, datasets\n",
    "import timm\n",
    "\n",
    "# --- 1. Load Your Configurations ---\n",
    "DATA_DIR = \"dataset\"\n",
    "IMG_SIZE = 224\n",
    "DEVICE = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAMES = ['fastvit_t8']\n",
    "\n",
    "# --- 2. Recreate Validation DataLoader (Same as Training) ---\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load class mappings (assuming same as training)\n",
    "full_dataset = datasets.ImageFolder(root=DATA_DIR)\n",
    "idx_to_class = {v: k for k, v in full_dataset.class_to_idx.items()}\n",
    "NUM_CLASSES = len(full_dataset.classes)\n",
    "\n",
    "# --- 3. Evaluation Function ---\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    # Convert numpy arrays to lists with native Python types\n",
    "    all_preds = [int(x) for x in all_preds]\n",
    "    all_labels = [int(x) for x in all_labels]\n",
    "    all_probs = [list(map(float, x)) for x in all_probs]\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = float(accuracy_score(all_labels, all_preds))\n",
    "    precision = float(precision_score(all_labels, all_preds, average=\"weighted\"))\n",
    "    recall = float(recall_score(all_labels, all_preds, average=\"weighted\"))\n",
    "    f1 = float(f1_score(all_labels, all_preds, average=\"weighted\"))\n",
    "    cm = confusion_matrix(all_labels, all_preds).tolist()\n",
    "    cls_report = classification_report(all_labels, all_preds, target_names=list(full_dataset.class_to_idx.keys()), output_dict=True)\n",
    "    \n",
    "    # ROC-AUC (for multi-class)\n",
    "    try:\n",
    "        roc_auc = float(roc_auc_score(all_labels, all_probs, multi_class=\"ovo\", average=\"weighted\"))\n",
    "    except:\n",
    "        roc_auc = None  # Skip if too many classes or other issues\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"classification_report\": cls_report,\n",
    "        \"predictions\": all_preds,\n",
    "        \"true_labels\": all_labels,\n",
    "        \"class_probabilities\": all_probs\n",
    "    }\n",
    "\n",
    "# --- 4. Visualization Functions ---\n",
    "def plot_confusion_matrix(cm, class_names, model_name):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=class_names, \n",
    "                yticklabels=class_names)\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.savefig(f\"{model_name}_confusion_matrix.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# --- 5. Main Evaluation Loop ---\n",
    "def main():\n",
    "    # Load validation data (same split as during training)\n",
    "    val_dataset = datasets.ImageFolder(root=DATA_DIR, transform=val_transforms)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "    all_results = {}\n",
    "    \n",
    "    for model_name in MODEL_NAMES:\n",
    "        print(f\"\\n=== Evaluating {model_name} ===\")\n",
    "        \n",
    "        # Load model architecture\n",
    "        model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)\n",
    "        model.name = model_name\n",
    "        \n",
    "        # Load trained weights\n",
    "        model_path = f\"trained_models/{model_name}_best.pth\"  # Update this line in the script\n",
    "        if os.path.exists(model_path):\n",
    "            model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "            model = model.to(DEVICE)\n",
    "            \n",
    "            # Evaluate\n",
    "            results = evaluate_model(model, val_loader)\n",
    "            all_results[model_name] = results\n",
    "            \n",
    "            # Save metrics\n",
    "            with open(f\"{model_name}_metrics.json\", \"w\") as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "            \n",
    "            # Visualizations\n",
    "            plot_confusion_matrix(\n",
    "                np.array(results[\"confusion_matrix\"]),\n",
    "                list(full_dataset.class_to_idx.keys()),\n",
    "                model_name\n",
    "            )\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\"\\nResults for {model_name}:\")\n",
    "            print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "            print(f\"Precision: {results['precision']:.4f}\")\n",
    "            print(f\"Recall: {results['recall']:.4f}\")\n",
    "            print(f\"F1-Score: {results['f1_score']:.4f}\")\n",
    "            if results['roc_auc']:\n",
    "                print(f\"ROC-AUC: {results['roc_auc']:.4f}\")\n",
    "            print(\"\\nClassification Report:\\n\", results[\"classification_report\"])\n",
    "        else:\n",
    "            print(f\"Model weights not found at {model_path}. Skipping...\")\n",
    "    \n",
    "    # Compare all models\n",
    "    print(\"\\n=== Model Comparison ===\")\n",
    "    comparison_df = pd.DataFrame.from_dict({\n",
    "        model: {\n",
    "            'Accuracy': results['accuracy'],\n",
    "            'Precision': results['precision'],\n",
    "            'Recall': results['recall'],\n",
    "            'F1-Score': results['f1_score'],\n",
    "            'ROC-AUC': results['roc_auc'] if results['roc_auc'] else None\n",
    "        }\n",
    "        for model, results in all_results.items()\n",
    "    }, orient='index')\n",
    "    \n",
    "    print(comparison_df)\n",
    "    comparison_df.to_csv(\"model_comparison.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fe0eab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
