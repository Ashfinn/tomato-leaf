{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848aa358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Add missing imports\n",
    "from torchvision import transforms, datasets\n",
    "import timm\n",
    "\n",
    "# --- 1. Load Your Configurations ---\n",
    "DATA_DIR = \"dataset\"\n",
    "IMG_SIZE = 240\n",
    "DEVICE = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAMES = ['crossvit_tiny_240']\n",
    "\n",
    "# --- 2. Recreate Validation DataLoader (Same as Training) ---\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load class mappings (assuming same as training)\n",
    "full_dataset = datasets.ImageFolder(root=DATA_DIR)\n",
    "idx_to_class = {v: k for k, v in full_dataset.class_to_idx.items()}\n",
    "NUM_CLASSES = len(full_dataset.classes)\n",
    "\n",
    "# --- 3. Evaluation Function ---\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    # Convert numpy arrays to lists with native Python types\n",
    "    all_preds = [int(x) for x in all_preds]\n",
    "    all_labels = [int(x) for x in all_labels]\n",
    "    all_probs = [list(map(float, x)) for x in all_probs]\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = float(accuracy_score(all_labels, all_preds))\n",
    "    precision = float(precision_score(all_labels, all_preds, average=\"weighted\"))\n",
    "    recall = float(recall_score(all_labels, all_preds, average=\"weighted\"))\n",
    "    f1 = float(f1_score(all_labels, all_preds, average=\"weighted\"))\n",
    "    cm = confusion_matrix(all_labels, all_preds).tolist()\n",
    "    cls_report = classification_report(all_labels, all_preds, target_names=list(full_dataset.class_to_idx.keys()), output_dict=True)\n",
    "    \n",
    "    # ROC-AUC (for multi-class)\n",
    "    try:\n",
    "        roc_auc = float(roc_auc_score(all_labels, all_probs, multi_class=\"ovo\", average=\"weighted\"))\n",
    "    except:\n",
    "        roc_auc = None  # Skip if too many classes or other issues\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"classification_report\": cls_report,\n",
    "        \"predictions\": all_preds,\n",
    "        \"true_labels\": all_labels,\n",
    "        \"class_probabilities\": all_probs\n",
    "    }\n",
    "\n",
    "# --- 4. Visualization Functions ---\n",
    "def plot_confusion_matrix(cm, class_names, model_name):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=class_names, \n",
    "                yticklabels=class_names)\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.savefig(f\"{model_name}_confusion_matrix.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# --- 5. Main Evaluation Loop ---\n",
    "def main():\n",
    "    # Load validation data (same split as during training)\n",
    "    val_dataset = datasets.ImageFolder(root=DATA_DIR, transform=val_transforms)\n",
    "    \n",
    "    # Recreate the exact same train/val split as training\n",
    "    random.seed(42)  # Same seed as training\n",
    "    indices = list(range(len(val_dataset)))\n",
    "    random.shuffle(indices)\n",
    "    split_point = int(0.8 * len(indices))  # Same ratio as training\n",
    "    val_indices = indices[split_point:]  # Only validation indices\n",
    "\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, sampler=val_sampler, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    print(f\"Evaluating on {len(val_indices)} validation samples out of {len(val_dataset)} total samples\")\n",
    "\n",
    "    all_results = {}\n",
    "    \n",
    "    for model_name in MODEL_NAMES:\n",
    "        print(f\"\\n=== Evaluating {model_name} ===\")\n",
    "        \n",
    "        # Load model architecture\n",
    "        model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)\n",
    "        model.name = model_name\n",
    "        \n",
    "        # Load trained weights\n",
    "        model_path = f\"trained_models/{model_name}_best.pth\"\n",
    "        if os.path.exists(model_path):\n",
    "            model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "            model = model.to(DEVICE)\n",
    "            \n",
    "            # Evaluate\n",
    "            results = evaluate_model(model, val_loader)\n",
    "            all_results[model_name] = results\n",
    "            \n",
    "            # Save metrics\n",
    "            with open(f\"{model_name}_metrics.json\", \"w\") as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "            \n",
    "            # Visualizations\n",
    "            plot_confusion_matrix(\n",
    "                np.array(results[\"confusion_matrix\"]),\n",
    "                list(full_dataset.class_to_idx.keys()),\n",
    "                model_name\n",
    "            )\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\"\\nResults for {model_name}:\")\n",
    "            print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "            print(f\"Precision: {results['precision']:.4f}\")\n",
    "            print(f\"Recall: {results['recall']:.4f}\")\n",
    "            print(f\"F1-Score: {results['f1_score']:.4f}\")\n",
    "            if results['roc_auc']:\n",
    "                print(f\"ROC-AUC: {results['roc_auc']:.4f}\")\n",
    "            print(f\"\\nClassification Report:\")\n",
    "            for class_name, metrics in results[\"classification_report\"].items():\n",
    "                if isinstance(metrics, dict) and 'precision' in metrics:\n",
    "                    print(f\"{class_name}: Precision={metrics['precision']:.3f}, Recall={metrics['recall']:.3f}, F1={metrics['f1-score']:.3f}\")\n",
    "        else:\n",
    "            print(f\"Model weights not found at {model_path}. Skipping...\")\n",
    "    \n",
    "    # Compare all models\n",
    "    if all_results:\n",
    "        print(\"\\n=== Model Comparison ===\")\n",
    "        comparison_df = pd.DataFrame.from_dict({\n",
    "            model: {\n",
    "                'Accuracy': results['accuracy'],\n",
    "                'Precision': results['precision'],\n",
    "                'Recall': results['recall'],\n",
    "                'F1-Score': results['f1_score'],\n",
    "                'ROC-AUC': results['roc_auc'] if results['roc_auc'] else None\n",
    "            }\n",
    "            for model, results in all_results.items()\n",
    "        }, orient='index')\n",
    "        \n",
    "        print(comparison_df)\n",
    "        \n",
    "        # Save comparison to CSV\n",
    "        comparison_df.to_csv(\"model_comparison.csv\")\n",
    "        print(\"\\nModel comparison saved to model_comparison.csv\")\n",
    "    else:\n",
    "        print(\"No models were successfully evaluated.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eccdd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
