{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ExYTyHDXPxV",
      "metadata": {
        "id": "4ExYTyHDXPxV"
      },
      "outputs": [],
      "source": [
        "# ✅ 2. Install required packages\n",
        "!pip install tensorflow scikit-learn matplotlib seaborn pillow opencv-python --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "htPW8PUSa3Qw",
      "metadata": {
        "id": "htPW8PUSa3Qw"
      },
      "outputs": [],
      "source": [
        "rm -rf sample_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "_lLu2fnJaFWN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lLu2fnJaFWN",
        "outputId": "73050ca2-e473-49e2-cf49-0b0ae2fb42fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'tomato-leaf'...\n",
            "remote: Enumerating objects: 36105, done.\u001b[K\n",
            "remote: Counting objects: 100% (319/319), done.\u001b[K\n",
            "remote: Compressing objects: 100% (316/316), done.\u001b[K\n",
            "remote: Total 36105 (delta 4), reused 315 (delta 3), pack-reused 35786 (from 1)\u001b[K\n",
            "Receiving objects: 100% (36105/36105), 538.25 MiB | 23.20 MiB/s, done.\n",
            "Resolving deltas: 100% (1373/1373), done.\n",
            "Updating files: 100% (32035/32035), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Ashfinn/tomato-leaf.git"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oMUfc7EUVzzU",
      "metadata": {
        "id": "oMUfc7EUVzzU"
      },
      "source": [
        "# Comprehensive list of models for comparative analysis\n",
        "# Modern CNNs\n",
        "```\n",
        "    'poolformer_s12',               # PoolFormer - simple, efficient CNN alternative\n",
        "    \n",
        "    # Hybrid CNN-Transformers\n",
        "    'coat_lite_tiny',               # CoAtNet-Lite - combines conv and self-attention\n",
        "    'levit_128s',                   # LeViT - efficient hybrid CNN/ViT\n",
        "    \n",
        "    # Pure Vision Transformers\n",
        "    'vit_tiny_patch16_224',         # Vanilla ViT - smallest standard ViT\n",
        "    'vit_small_patch16_224',        # Vanilla ViT - slightly larger\n",
        "    'beit_base_patch16_224',        # BEiT - masked image modeling pre-trained\n",
        "    'crossvit_tiny_240',            # CrossViT - multiple size patches\n",
        "    'pvt_tiny',                     # Pyramid Vision Transformer\n",
        "    'twins_pcpvt_base',             # Twins - local-global attention\n",
        "    'xcit_tiny_12_224',             # XCiT - cross-covariance attention\n",
        "    'tnt_s_patch16_224',            # TNT - Transformer in Transformer\n",
        "    \n",
        "    # # Efficient/Edge Models\n",
        "    'maxvit_nano_rw_256',           # MaxViT - Max-Attention for efficiency\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cEQvMXgx2qx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cEQvMXgx2qx",
        "outputId": "9a46284c-2832-4a1d-a243-86cb528fdd6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Found 10 classes: ['Tomato_Bacterial_spot', 'Tomato_Early_blight', 'Tomato_Late_blight', 'Tomato_Leaf_Mold', 'Tomato_Septoria_leaf_spot', 'Tomato_Spider_mites_Two_spotted_spider_mite', 'Tomato__Target_Spot', 'Tomato__Tomato_YellowLeaf__Curl_Virus', 'Tomato__Tomato_mosaic_virus', 'Tomato_healthy']\n",
            "Data split into 12808 training images and 3203 validation images.\n",
            "\n",
            "--- Preparing DataLoaders ---\n",
            "\n",
            "--- Starting fine-tuning for maxvit_nano_rw_256 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded maxvit_nano_rw_256 with 10 classes.\n",
            "Starting training for 10 epochs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 (Train): 100%|██████████| 401/401 [03:46<00:00,  1.77it/s, loss=1.18]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Train Loss: 1.3394, Train Acc: 0.5411\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 (Val): 100%|██████████| 101/101 [00:18<00:00,  5.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Val Loss: 0.9118, Val Acc: 0.6460\n",
            "Validation accuracy improved. Saving best model for maxvit_nano_rw_256.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 (Train): 100%|██████████| 401/401 [03:46<00:00,  1.77it/s, loss=0.517]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Train Loss: 0.5956, Train Acc: 0.8033\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 (Val): 100%|██████████| 101/101 [00:18<00:00,  5.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Val Loss: 0.5373, Val Acc: 0.8002\n",
            "Validation accuracy improved. Saving best model for maxvit_nano_rw_256.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 (Train): 100%|██████████| 401/401 [03:45<00:00,  1.78it/s, loss=0.817]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Train Loss: 0.4423, Train Acc: 0.8542\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 (Val): 100%|██████████| 101/101 [00:17<00:00,  5.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Val Loss: 0.3124, Val Acc: 0.8976\n",
            "Validation accuracy improved. Saving best model for maxvit_nano_rw_256.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 (Train): 100%|██████████| 401/401 [03:45<00:00,  1.78it/s, loss=0.397]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Train Loss: 0.3631, Train Acc: 0.8799\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 (Val): 100%|██████████| 101/101 [00:17<00:00,  5.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Val Loss: 0.2229, Val Acc: 0.9204\n",
            "Validation accuracy improved. Saving best model for maxvit_nano_rw_256.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 (Train):  56%|█████▌    | 223/401 [02:06<01:39,  1.79it/s, loss=0.487]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split, SubsetRandomSampler\n",
        "from PIL import Image\n",
        "import os\n",
        "import timm\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "IMG_SIZE = 256 # Standard input size for most pre-trained models\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Updated list of the 5 chosen models (EXCLUDING MobileNet, EfficientNet, GhostNet, Inception, Xception, ShuffleNet)\n",
        "MODEL_NAMES = ['maxvit_nano_rw_256']\n",
        "\n",
        "# --- 2. Data Preparation and Splitting ---\n",
        "\n",
        "# Ensure DATA_DIR points to the parent folder of your disease subfolders\n",
        "DATA_DIR = \"tomato-leaf/dataset\" # Confirmed this is your structure based on screenshot\n",
        "\n",
        "# Define data transformations for training and validation\n",
        "# Training transforms include data augmentation to improve model robustness\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMG_SIZE),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # ImageNet mean/std\n",
        "])\n",
        "\n",
        "# Validation transforms do not include augmentation, only necessary resizing and normalization\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),       # Resize to a larger size first\n",
        "    transforms.CenterCrop(IMG_SIZE), # Then crop the center\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # ImageNet mean/std\n",
        "])\n",
        "\n",
        "# Load the full dataset (without transforms initially, just to get class info and paths)\n",
        "full_dataset_no_transform = datasets.ImageFolder(root=DATA_DIR)\n",
        "\n",
        "NUM_CLASSES = len(full_dataset_no_transform.classes)\n",
        "class_to_idx = full_dataset_no_transform.class_to_idx\n",
        "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
        "print(f\"Found {NUM_CLASSES} classes: {list(class_to_idx.keys())}\")\n",
        "\n",
        "# Define the split ratio\n",
        "TRAIN_SPLIT_RATIO = 0.8\n",
        "\n",
        "# Get indices for splitting\n",
        "dataset_size = len(full_dataset_no_transform)\n",
        "indices = list(range(dataset_size))\n",
        "# Shuffle indices for a truly random split\n",
        "import random\n",
        "random.seed(42) # for reproducibility\n",
        "random.shuffle(indices)\n",
        "\n",
        "split_point = int(TRAIN_SPLIT_RATIO * dataset_size)\n",
        "train_indices, val_indices = indices[:split_point], indices[split_point:]\n",
        "\n",
        "# Create samplers\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "val_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "# Create two ImageFolder instances with their respective transforms, pointing to the same data\n",
        "train_dataset = datasets.ImageFolder(root=DATA_DIR, transform=train_transforms)\n",
        "val_dataset = datasets.ImageFolder(root=DATA_DIR, transform=val_transforms)\n",
        "\n",
        "# Create DataLoaders using the samplers and their respective datasets\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, sampler=val_sampler, num_workers=4, pin_memory=True)\n",
        "\n",
        "print(f\"Data split into {len(train_indices)} training images and {len(val_indices)} validation images.\")\n",
        "\n",
        "\n",
        "# --- 3. Model Loading and Fine-tuning Setup ---\n",
        "def load_and_prepare_model(model_name, num_classes, pretrained=True):\n",
        "    # For some transformer models, `pretrained_cfg` might not always be directly available\n",
        "    # or fully define the transforms needed. It's safer to define your own based on common\n",
        "    # ImageNet practices as done in train_transforms/val_transforms.\n",
        "    # We still use pretrained=True to load weights, but manage transforms manually.\n",
        "    model = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes)\n",
        "    model = model.to(DEVICE)\n",
        "    print(f\"Loaded {model_name} with {num_classes} classes.\")\n",
        "    return model\n",
        "\n",
        "# --- 4. Training Function ---\n",
        "def train_model(model, train_loader, val_loader, num_epochs, learning_rate):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "    print(f\"Starting training for {num_epochs} epochs...\")\n",
        "    best_val_accuracy = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Train)\")\n",
        "        for images, labels in train_loop:\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_samples += labels.size(0)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "            train_loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        epoch_loss = running_loss / total_samples\n",
        "        epoch_accuracy = correct_predictions / total_samples\n",
        "        print(f\"Epoch {epoch+1} Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_accuracy:.4f}\")\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct_predictions = 0\n",
        "        val_total_samples = 0\n",
        "        with torch.no_grad():\n",
        "            val_loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Val)\")\n",
        "            for images, labels in val_loop:\n",
        "                images = images.to(DEVICE)\n",
        "                labels = labels.to(DEVICE)\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total_samples += labels.size(0)\n",
        "                val_correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "        val_epoch_loss = val_loss / val_total_samples\n",
        "        val_epoch_accuracy = val_correct_predictions / val_total_samples\n",
        "        print(f\"Epoch {epoch+1} Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_accuracy:.4f}\")\n",
        "\n",
        "        scheduler.step(val_epoch_loss)\n",
        "\n",
        "        if val_epoch_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_epoch_accuracy\n",
        "            # For saving the model name: model.name is assigned just before calling train_model\n",
        "            torch.save(model.state_dict(), f\"{model.name}_best.pth\")\n",
        "            print(f\"Validation accuracy improved. Saving best model for {model.name}.\")\n",
        "\n",
        "    print(\"Training finished!\")\n",
        "    return model\n",
        "\n",
        "# --- 5. Inference Function ---\n",
        "def predict_image(model, image_path, transform, idx_to_class):\n",
        "    model.eval()\n",
        "\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transform(image).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image_tensor)\n",
        "        probabilities = torch.softmax(outputs, dim=1)\n",
        "        _, predicted_idx = torch.max(probabilities, 1)\n",
        "\n",
        "    predicted_class = idx_to_class[predicted_idx.item()]\n",
        "    confidence = probabilities[0][predicted_idx.item()].item()\n",
        "\n",
        "    return predicted_class, confidence\n",
        "\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n--- Preparing DataLoaders ---\")\n",
        "\n",
        "    # The data loaders train_loader and val_loader are created above the main loop\n",
        "    # and will be reused for all models, ensuring consistent splits and transforms.\n",
        "\n",
        "    for model_name in MODEL_NAMES:\n",
        "        print(f\"\\n--- Starting fine-tuning for {model_name} ---\")\n",
        "\n",
        "        model = load_and_prepare_model(model_name, NUM_CLASSES, pretrained=True)\n",
        "        model.name = model_name # Assign name attribute for saving\n",
        "\n",
        "        train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=0.001)\n",
        "\n",
        "        print(f\"Loading best weights for {model_name}...\")\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(f\"{model.name}_best.pth\"))\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: No best model saved for {model.name}. Using last epoch's weights.\")\n",
        "\n",
        "        print(f\"\\n--- Performing inference with {model.name} ---\")\n",
        "        if len(val_indices) > 0:\n",
        "            # val_dataset.imgs is a list of (image_path, class_idx) tuples for the full dataset\n",
        "            # val_indices gives us the indices of images that belong to the validation set\n",
        "            sample_image_path = val_dataset.imgs[val_indices[0]][0]\n",
        "            print(f\"Using sample image: {os.path.basename(sample_image_path)}\") # Print just filename for brevity\n",
        "            inference_transform = val_transforms\n",
        "\n",
        "            predicted_class, confidence = predict_image(model, sample_image_path, inference_transform, idx_to_class)\n",
        "            print(f\"Predicted Class: {predicted_class}\")\n",
        "            print(f\"Confidence: {confidence:.4f}\")\n",
        "        else:\n",
        "            print(\"No validation images available for inference demonstration.\")\n",
        "        print(\"-\" * 50)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
