{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tomato Leaf Disease Detection: InceptionV3 Optimized for Ryzen 5 5600G\n",
    "\n",
    "### 1. Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-cpu\n",
      "  Using cached tensorflow_cpu-2.19.0-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-cpu) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-cpu) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-cpu) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-cpu) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-cpu) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-cpu) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-cpu) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-cpu) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-cpu) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-cpu) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-cpu) (75.8.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-cpu) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-cpu) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-cpu) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-cpu) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-cpu) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-cpu) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-cpu) (3.9.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-cpu) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-cpu) (0.5.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-cpu) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-cpu) (14.0.0)\n",
      "Requirement already satisfied: namex in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-cpu) (0.0.9)\n",
      "Requirement already satisfied: optree in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-cpu) (0.15.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-cpu) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-cpu) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-cpu) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-cpu) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow-cpu) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow-cpu) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow-cpu) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow-cpu) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-cpu) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-cpu) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\obidur rahman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-cpu) (0.1.2)\n",
      "Using cached tensorflow_cpu-2.19.0-cp312-cp312-win_amd64.whl (376.0 MB)\n",
      "Installing collected packages: tensorflow-cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Obidur Rahman\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\tensorflow\\\\compiler\\\\mlir\\\\lite\\\\python\\\\_pywrap_converter_api.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-cpu opencv-python matplotlib seaborn numpy\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "# Enable mixed precision\n",
    "set_global_policy('mixed_float16')\n",
    "\n",
    "# Configure TensorFlow for CPU (12 threads)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(12)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected classes: ['Tomato_Bacterial_spot', 'Tomato_Early_blight', 'Tomato_Late_blight', 'Tomato_Leaf_Mold', 'Tomato_Septoria_leaf_spot', 'Tomato_Spider_mites_Two_spotted_spider_mite', 'Tomato__Target_Spot', 'Tomato__Tomato_YellowLeaf__Curl_Virus', 'Tomato__Tomato_mosaic_virus', 'Tomato_healthy']\n",
      "Total number of classes: 10\n",
      "Class 'Tomato_Bacterial_spot': 2127 images\n",
      "Class 'Tomato_Early_blight': 1000 images\n",
      "Class 'Tomato_Late_blight': 1909 images\n",
      "Class 'Tomato_Leaf_Mold': 952 images\n",
      "Class 'Tomato_Septoria_leaf_spot': 1771 images\n",
      "Class 'Tomato_Spider_mites_Two_spotted_spider_mite': 1676 images\n",
      "Class 'Tomato__Target_Spot': 1404 images\n",
      "Class 'Tomato__Tomato_YellowLeaf__Curl_Virus': 3209 images\n",
      "Class 'Tomato__Tomato_mosaic_virus': 373 images\n",
      "Class 'Tomato_healthy': 1591 images\n",
      "Total images in dataset: 16012\n"
     ]
    }
   ],
   "source": [
    "# Global constants\n",
    "batch_size = 8\n",
    "TRAIN_PERCENT = 0.8\n",
    "img_size = (150, 150)\n",
    "\n",
    "dataset_dir = \"dataset\"\n",
    "\n",
    "# Get class names\n",
    "class_names = sorted([d for d in os.listdir(dataset_dir) \n",
    "                     if os.path.isdir(os.path.join(dataset_dir, d)) and not d.startswith('.')])\n",
    "print(f\"Detected classes: {class_names}\")\n",
    "print(f\"Total number of classes: {len(class_names)}\")\n",
    "\n",
    "# Count total images\n",
    "total_images = 0\n",
    "for class_name in class_names:\n",
    "    class_dir = os.path.join(dataset_dir, class_name)\n",
    "    num_images = len([f for f in os.listdir(class_dir) \n",
    "                     if os.path.isfile(os.path.join(class_dir, f))])\n",
    "    print(f\"Class '{class_name}': {num_images} images\")\n",
    "    total_images += num_images\n",
    "print(f\"Total images in dataset: {total_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Loading & Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class Tomato_Bacterial_spot... (2127 images)\n",
      "Processing class Tomato_Early_blight... (1000 images)\n",
      "Processing class Tomato_Late_blight... (1909 images)\n",
      "Processing class Tomato_Leaf_Mold... (952 images)\n",
      "Processing class Tomato_Septoria_leaf_spot... (1771 images)\n",
      "Processing class Tomato_Spider_mites_Two_spotted_spider_mite... (1676 images)\n",
      "Processing class Tomato__Target_Spot... (1404 images)\n",
      "Processing class Tomato__Tomato_YellowLeaf__Curl_Virus... (3208 images)\n",
      "Processing class Tomato__Tomato_mosaic_virus... (373 images)\n",
      "Processing class Tomato_healthy... (1591 images)\n",
      "Dataset loaded: ~16012 images, (150, 150) resolution\n",
      "Training set: ~12809 images\n",
      "Testing set: ~3203 images\n"
     ]
    }
   ],
   "source": [
    "def hybrid_segmentation(img):\n",
    "    # Simplified segmentation\n",
    "    img = img.numpy()  # Convert tensor to NumPy for OpenCV\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    lower_green = np.array([25, 40, 50])\n",
    "    upper_green = np.array([85, 255, 255])\n",
    "    mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "\n",
    "    # Fallback to full image if segmentation fails\n",
    "    if np.sum(mask) < 0.2 * mask.size:\n",
    "        mask = np.ones(img.shape[:2], np.uint8) * 255\n",
    "\n",
    "    segmented = cv2.bitwise_and(img, img, mask=mask)\n",
    "    return segmented\n",
    "\n",
    "def process_image(file_path, label, img_size=(150, 150)):\n",
    "    # Load and preprocess a single image\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)  # Adjust for PNG if needed\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)  # Use float32\n",
    "    img = tf.image.resize(img, img_size)\n",
    "    img = tf.py_function(func=hybrid_segmentation, inp=[img], Tout=tf.float32)\n",
    "    img.set_shape([img_size[0], img_size[1], 3])\n",
    "    return img, label\n",
    "\n",
    "# Data augmentation layers\n",
    "augmentation = models.Sequential([\n",
    "    layers.RandomRotation(0.0417),  # ~15 degrees (15/360)\n",
    "    layers.RandomTranslation(0.1, 0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomFlip(\"horizontal\")\n",
    "])\n",
    "\n",
    "def create_dataset(dataset_dir, class_names, img_size=(150, 150), train_percent=0.8):\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(dataset_dir, class_name)\n",
    "        files = [os.path.join(class_dir, f) for f in os.listdir(class_dir) \n",
    "                 if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        file_paths.extend(files)\n",
    "        labels.extend([class_idx] * len(files))\n",
    "        print(f\"Processing class {class_name}... ({len(files)} images)\")\n",
    "\n",
    "    # Convert to tensors\n",
    "    file_paths = tf.constant(file_paths)\n",
    "    labels = tf.constant(labels)\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "    dataset = dataset.shuffle(buffer_size=len(file_paths), seed=42)\n",
    "    dataset = dataset.map(process_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Split into train and test\n",
    "    total_size = len(file_paths)\n",
    "    train_size = int(train_percent * total_size)\n",
    "    train_dataset = dataset.take(train_size)\n",
    "    test_dataset = dataset.skip(train_size)\n",
    "\n",
    "    # Apply augmentation to training dataset\n",
    "    train_dataset = train_dataset.map(lambda x, y: (augmentation(x, training=True), y), \n",
    "                                     num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Batch and prefetch\n",
    "    train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, test_dataset, labels\n",
    "\n",
    "# Load datasets\n",
    "train_dataset, test_dataset, y_labels = create_dataset(dataset_dir, class_names, img_size)\n",
    "print(f\"Dataset loaded: ~{total_images} images, {img_size} resolution\")\n",
    "print(f\"Training set: ~{int(total_images * TRAIN_PERCENT)} images\")\n",
    "print(f\"Testing set: ~{total_images - int(total_images * TRAIN_PERCENT)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Deep Learning Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training InceptionV3...\n",
      "Epoch 1/15\n",
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.3490 - loss: 2.0892"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 190ms/step - accuracy: 0.3491 - loss: 2.0890 - val_accuracy: 0.6781 - val_loss: 0.9592\n",
      "Epoch 2/15\n",
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.5931 - loss: 1.2275"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 182ms/step - accuracy: 0.5931 - loss: 1.2275 - val_accuracy: 0.7393 - val_loss: 0.8007\n",
      "Epoch 3/15\n",
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.6496 - loss: 1.0649"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 183ms/step - accuracy: 0.6496 - loss: 1.0649 - val_accuracy: 0.7590 - val_loss: 0.6940\n",
      "Epoch 4/15\n",
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.6711 - loss: 0.9899"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 182ms/step - accuracy: 0.6711 - loss: 0.9899 - val_accuracy: 0.7827 - val_loss: 0.6659\n",
      "Epoch 5/15\n",
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.6907 - loss: 0.9294"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 183ms/step - accuracy: 0.6907 - loss: 0.9294 - val_accuracy: 0.7886 - val_loss: 0.6405\n",
      "Epoch 6/15\n",
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.7010 - loss: 0.8910"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 183ms/step - accuracy: 0.7010 - loss: 0.8910 - val_accuracy: 0.8021 - val_loss: 0.5764\n",
      "Epoch 7/15\n",
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 183ms/step - accuracy: 0.7057 - loss: 0.8695 - val_accuracy: 0.8052 - val_loss: 0.5889\n",
      "Epoch 8/15\n",
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.7153 - loss: 0.8657"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 184ms/step - accuracy: 0.7153 - loss: 0.8656 - val_accuracy: 0.8261 - val_loss: 0.5335\n",
      "Epoch 9/15\n",
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 204ms/step - accuracy: 0.7187 - loss: 0.8206 - val_accuracy: 0.8139 - val_loss: 0.5393\n",
      "Epoch 10/15\n",
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 193ms/step - accuracy: 0.7216 - loss: 0.8268 - val_accuracy: 0.8133 - val_loss: 0.5556\n",
      "Epoch 11/15\n",
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.7336 - loss: 0.7850"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 183ms/step - accuracy: 0.7336 - loss: 0.7850 - val_accuracy: 0.8186 - val_loss: 0.5286\n",
      "Epoch 12/15\n",
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.7322 - loss: 0.7886"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 184ms/step - accuracy: 0.7322 - loss: 0.7886 - val_accuracy: 0.8308 - val_loss: 0.5139\n",
      "Epoch 13/15\n",
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.7311 - loss: 0.7863"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 194ms/step - accuracy: 0.7311 - loss: 0.7863 - val_accuracy: 0.8320 - val_loss: 0.5091\n",
      "Epoch 14/15\n",
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.7464 - loss: 0.7594"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 204ms/step - accuracy: 0.7464 - loss: 0.7594 - val_accuracy: 0.8345 - val_loss: 0.4849\n",
      "Epoch 15/15\n",
      "\u001b[1m1601/1601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 190ms/step - accuracy: 0.7409 - loss: 0.7584 - val_accuracy: 0.8336 - val_loss: 0.5144\n",
      "InceptionV3 Test Accuracy: 0.8336\n"
     ]
    }
   ],
   "source": [
    "def build_inceptionv3_model(input_shape=(150, 150, 3), num_classes=len(class_names)):\n",
    "    base = InceptionV3(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    base.trainable = False\n",
    "    model = models.Sequential([\n",
    "        base,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(num_classes, activation='softmax', dtype='float32')\n",
    "    ])\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize model\n",
    "model = build_inceptionv3_model()\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_inceptionv3.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train model\n",
    "print(\"Training InceptionV3...\")\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=15,\n",
    "    validation_data=test_dataset,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(test_dataset, verbose=0)\n",
    "print(f\"InceptionV3 Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Plot accuracy and loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_metrics.png')\n",
    "plt.close()\n",
    "\n",
    "# Confusion matrix\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for images, labels in test_dataset:\n",
    "    preds = model.predict(images, verbose=0)\n",
    "    y_pred.extend(np.argmax(preds, axis=1))\n",
    "    y_true.extend(labels.numpy())\n",
    "y_pred = np.array(y_pred)\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix - InceptionV3')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model Saving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('best_inceptionv3.h5')\n",
    "print(\"InceptionV3 model saved as best_inceptionv3.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
