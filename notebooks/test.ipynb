{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4200a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages for this notebook\n",
    "!pip install torch torchvision timm scikit-learn pandas seaborn matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922dc85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    cohen_kappa_score\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add missing imports\n",
    "from torchvision import transforms, datasets\n",
    "import timm\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_DIR = \"dataset\"\n",
    "TRAINED_MODELS_DIR = \"trained_models\"\n",
    "OUTPUT_DIR = \"evaluation_results\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64  # Increased for faster evaluation\n",
    "NUM_WORKERS = 8  # Increased for faster data loading\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Available CPU cores: {os.cpu_count()}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/confusion_matrices\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/metrics\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/visualizations\", exist_ok=True)\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def extract_image_size_from_model_name(model_name):\n",
    "    \"\"\"Extract image size from model name if present, otherwise return default.\"\"\"\n",
    "    # Look for patterns like _224, _240, _256, etc.\n",
    "    size_match = re.search(r'_(\\d{3})(?:_|\\.)', model_name)\n",
    "    if size_match:\n",
    "        return int(size_match.group(1))\n",
    "    \n",
    "    if '224' in model_name:\n",
    "        return 224\n",
    "\n",
    "    else:\n",
    "        return 128  # Default fallback\n",
    "\n",
    "def get_model_transforms(img_size):\n",
    "    \"\"\"Get appropriate transforms for given image size.\"\"\"\n",
    "    # Resize to slightly larger, then crop to exact size\n",
    "    resize_size = int(img_size * 1.14)  # Standard practice\n",
    "    \n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(resize_size),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def get_available_models():\n",
    "    \"\"\"Get all available model files from trained_models directory.\"\"\"\n",
    "    model_files = []\n",
    "    if os.path.exists(TRAINED_MODELS_DIR):\n",
    "        for file in os.listdir(TRAINED_MODELS_DIR):\n",
    "            if file.endswith('_best.pth'):\n",
    "                model_name = file.replace('_best.pth', '')\n",
    "                model_files.append(model_name)\n",
    "    return sorted(model_files)\n",
    "\n",
    "def calculate_model_size(model):\n",
    "    \"\"\"Calculate model size in MB.\"\"\"\n",
    "    param_size = 0\n",
    "    param_count = 0\n",
    "    for param in model.parameters():\n",
    "        param_count += param.numel()\n",
    "        param_size += param.numel() * param.element_size()\n",
    "    \n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.numel() * buffer.element_size()\n",
    "    \n",
    "    size_mb = (param_size + buffer_size) / 1024 / 1024\n",
    "    return size_mb, param_count\n",
    "\n",
    "def calculate_flops_estimate(model, img_size):\n",
    "    \"\"\"Rough FLOP estimation based on model parameters and input size.\"\"\"\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    # Very rough estimation: 2 * params * input_pixels\n",
    "    flops = 2 * param_count * img_size * img_size\n",
    "    return flops / 1e9  # Return in GFLOPs\n",
    "\n",
    "def measure_inference_time(model, dataloader, device, num_batches=10):\n",
    "    \"\"\"Measure average inference time.\"\"\"\n",
    "    model.eval()\n",
    "    times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images, _) in enumerate(dataloader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            \n",
    "            images = images.to(device, non_blocking=True)\n",
    "            \n",
    "            # Warm up\n",
    "            if i == 0:\n",
    "                _ = model(images)\n",
    "                continue\n",
    "            \n",
    "            start_time = time.time()\n",
    "            _ = model(images)\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "            \n",
    "            times.append(end_time - start_time)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    return avg_time * 1000  # Return in milliseconds\n",
    "\n",
    "# --- Evaluation Functions ---\n",
    "def evaluate_model_comprehensive(model, val_loader, model_name, img_size):\n",
    "    \"\"\"Comprehensive model evaluation with all metrics.\"\"\"\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    # Measure inference time\n",
    "    inference_time = measure_inference_time(model, val_loader, DEVICE)\n",
    "    \n",
    "    # Calculate model metrics\n",
    "    model_size_mb, param_count = calculate_model_size(model)\n",
    "    flops_estimate = calculate_flops_estimate(model, img_size)\n",
    "    \n",
    "    # Evaluation loop\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"Evaluating {model_name}\"):\n",
    "            images = images.to(DEVICE, non_blocking=True)\n",
    "            labels = labels.to(DEVICE, non_blocking=True)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    # Calculate all metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
    "    kappa = cohen_kappa_score(all_labels, all_preds)\n",
    "    \n",
    "    # Per-class metrics\n",
    "    precision_per_class = precision_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "    recall_per_class = recall_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "    f1_per_class = f1_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Classification report\n",
    "    cls_report = classification_report(all_labels, all_preds, \n",
    "                                       target_names=list(full_dataset.class_to_idx.keys()), \n",
    "                                       output_dict=True, zero_division=0)\n",
    "    \n",
    "    # ROC-AUC\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(all_labels, all_probs, multi_class=\"ovo\", average=\"weighted\")\n",
    "    except:\n",
    "        roc_auc = None\n",
    "    \n",
    "    # Calculate edge deployment metrics\n",
    "    # Efficiency score (higher is better): accuracy / (model_size * inference_time)\n",
    "    efficiency_score = accuracy / (model_size_mb * inference_time) if inference_time > 0 else 0\n",
    "    \n",
    "    # Throughput (images per second)\n",
    "    throughput = (BATCH_SIZE * 1000) / inference_time if inference_time > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"image_size\": img_size,\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1_score\": float(f1),\n",
    "        \"cohen_kappa\": float(kappa),\n",
    "        \"roc_auc\": float(roc_auc) if roc_auc else None,\n",
    "        \"model_size_mb\": float(model_size_mb),\n",
    "        \"parameter_count\": int(param_count),\n",
    "        \"estimated_flops_gflops\": float(flops_estimate),\n",
    "        \"inference_time_ms\": float(inference_time),\n",
    "        \"throughput_fps\": float(throughput),\n",
    "        \"efficiency_score\": float(efficiency_score),\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "        \"classification_report\": cls_report,\n",
    "        \"per_class_precision\": precision_per_class.tolist(),\n",
    "        \"per_class_recall\": recall_per_class.tolist(),\n",
    "        \"per_class_f1\": f1_per_class.tolist(),\n",
    "        \"predictions\": [int(x) for x in all_preds],\n",
    "        \"true_labels\": [int(x) for x in all_labels],\n",
    "        \"class_probabilities\": [[float(y) for y in x] for x in all_probs]\n",
    "    }\n",
    "\n",
    "# --- Visualization Functions ---\n",
    "def plot_confusion_matrix(cm, class_names, model_name):\n",
    "    \"\"\"Plot and save confusion matrix.\"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=class_names, \n",
    "                yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\", fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Predicted Class\", fontsize=12)\n",
    "    plt.ylabel(\"True Class\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/confusion_matrices/{model_name}_confusion_matrix.png\", \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_model_comparison(results_df):\n",
    "    \"\"\"Create comprehensive comparison visualizations.\"\"\"\n",
    "    \n",
    "    # 1. Performance Metrics Comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    results_df_sorted = results_df.sort_values('accuracy', ascending=True)\n",
    "    axes[0,0].barh(results_df_sorted['model_name'], results_df_sorted['accuracy'])\n",
    "    axes[0,0].set_title('Model Accuracy Comparison', fontweight='bold')\n",
    "    axes[0,0].set_xlabel('Accuracy')\n",
    "    \n",
    "    # F1-Score comparison\n",
    "    results_df_sorted = results_df.sort_values('f1_score', ascending=True)\n",
    "    axes[0,1].barh(results_df_sorted['model_name'], results_df_sorted['f1_score'])\n",
    "    axes[0,1].set_title('Model F1-Score Comparison', fontweight='bold')\n",
    "    axes[0,1].set_xlabel('F1-Score')\n",
    "    \n",
    "    # Model Size comparison\n",
    "    results_df_sorted = results_df.sort_values('model_size_mb', ascending=True)\n",
    "    axes[1,0].barh(results_df_sorted['model_name'], results_df_sorted['model_size_mb'])\n",
    "    axes[1,0].set_title('Model Size Comparison', fontweight='bold')\n",
    "    axes[1,0].set_xlabel('Model Size (MB)')\n",
    "    \n",
    "    # Inference Time comparison\n",
    "    results_df_sorted = results_df.sort_values('inference_time_ms', ascending=True)\n",
    "    axes[1,1].barh(results_df_sorted['model_name'], results_df_sorted['inference_time_ms'])\n",
    "    axes[1,1].set_title('Inference Time Comparison', fontweight='bold')\n",
    "    axes[1,1].set_xlabel('Inference Time (ms)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/visualizations/model_comparison_metrics.png\", \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Accuracy vs Efficiency Scatter Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(results_df['model_size_mb'], results_df['accuracy'], \n",
    "                          c=results_df['inference_time_ms'], cmap='viridis', \n",
    "                          s=100, alpha=0.7)\n",
    "    \n",
    "    # Add model names as labels\n",
    "    for i, model in enumerate(results_df['model_name']):\n",
    "        plt.annotate(model, (results_df.iloc[i]['model_size_mb'], results_df.iloc[i]['accuracy']),\n",
    "                     xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    plt.colorbar(scatter, label='Inference Time (ms)')\n",
    "    plt.xlabel('Model Size (MB)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Accuracy vs Size (Color = Inference Time)', fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/visualizations/accuracy_vs_size_scatter.png\", \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Efficiency Score Ranking\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    results_df_sorted = results_df.sort_values('efficiency_score', ascending=True)\n",
    "    bars = plt.barh(results_df_sorted['model_name'], results_df_sorted['efficiency_score'])\n",
    "    plt.title('Model Efficiency Score Ranking\\n(Accuracy / (Model Size × Inference Time))', \n",
    "              fontweight='bold')\n",
    "    plt.xlabel('Efficiency Score')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Color bars based on efficiency\n",
    "    colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(bars)))\n",
    "    for bar, color in zip(bars, colors):\n",
    "        bar.set_color(color)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/visualizations/efficiency_ranking.png\", \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Throughput vs Accuracy\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(results_df['throughput_fps'], results_df['accuracy'], \n",
    "                          c=results_df['model_size_mb'], cmap='plasma', \n",
    "                          s=100, alpha=0.7)\n",
    "    \n",
    "    for i, model in enumerate(results_df['model_name']):\n",
    "        plt.annotate(model, (results_df.iloc[i]['throughput_fps'], results_df.iloc[i]['accuracy']),\n",
    "                     xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    plt.colorbar(scatter, label='Model Size (MB)')\n",
    "    plt.xlabel('Throughput (FPS)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Throughput vs Accuracy (Color = Model Size)', fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/visualizations/throughput_vs_accuracy.png\", \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def create_detailed_report(results_df, class_names):\n",
    "    \"\"\"Create a detailed markdown report.\"\"\"\n",
    "    report = f\"\"\"# Comparative Analysis of Lightweight Vision Models for Tomato Disease Classification\n",
    "\n",
    "## Executive Summary\n",
    "This report presents a comprehensive evaluation of {len(results_df)} lightweight vision models for tomato disease classification, focusing on edge deployment capabilities.\n",
    "\n",
    "## Dataset Information\n",
    "- **Classes**: {len(class_names)} disease categories\n",
    "- **Class Names**: {', '.join(class_names)}\n",
    "- **Evaluation Split**: 20% validation set (consistent across all models)\n",
    "\n",
    "## Model Performance Summary\n",
    "\n",
    "### Top 5 Models by Accuracy\n",
    "{results_df.nlargest(5, 'accuracy')[['model_name', 'accuracy', 'f1_score', 'model_size_mb', 'inference_time_ms']].to_markdown(index=False, floatfmt='.4f')}\n",
    "\n",
    "### Top 5 Models by Efficiency Score\n",
    "{results_df.nlargest(5, 'efficiency_score')[['model_name', 'efficiency_score', 'accuracy', 'model_size_mb', 'inference_time_ms']].to_markdown(index=False, floatfmt='.4f')}\n",
    "\n",
    "### Top 5 Fastest Models (Lowest Inference Time)\n",
    "{results_df.nsmallest(5, 'inference_time_ms')[['model_name', 'inference_time_ms', 'throughput_fps', 'accuracy', 'model_size_mb']].to_markdown(index=False, floatfmt='.4f')}\n",
    "\n",
    "### Top 5 Smallest Models\n",
    "{results_df.nsmallest(5, 'model_size_mb')[['model_name', 'model_size_mb', 'parameter_count', 'accuracy', 'inference_time_ms']].to_markdown(index=False, floatfmt='.4f')}\n",
    "\n",
    "## Complete Results\n",
    "{results_df.to_markdown(index=False, floatfmt='.4f')}\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "1. **Best Overall Performance**: {results_df.loc[results_df['accuracy'].idxmax(), 'model_name']} achieved the highest accuracy of {results_df['accuracy'].max():.4f}\n",
    "\n",
    "2. **Most Efficient**: {results_df.loc[results_df['efficiency_score'].idxmax(), 'model_name']} offers the best efficiency score of {results_df['efficiency_score'].max():.4f}\n",
    "\n",
    "3. **Fastest Inference**: {results_df.loc[results_df['inference_time_ms'].idxmin(), 'model_name']} has the lowest inference time of {results_df['inference_time_ms'].min():.2f}ms\n",
    "\n",
    "4. **Smallest Model**: {results_df.loc[results_df['model_size_mb'].idxmin(), 'model_name']} is the most compact at {results_df['model_size_mb'].min():.2f}MB\n",
    "\n",
    "## Recommendations for Edge Deployment\n",
    "\n",
    "Based on the analysis, the following models are recommended for different deployment scenarios:\n",
    "\n",
    "- **High Accuracy Priority**: {results_df.nlargest(1, 'accuracy')['model_name'].iloc[0]}\n",
    "- **Balanced Performance**: {results_df.nlargest(1, 'efficiency_score')['model_name'].iloc[0]}\n",
    "- **Speed Priority**: {results_df.nsmallest(1, 'inference_time_ms')['model_name'].iloc[0]}\n",
    "- **Memory Constrained**: {results_df.nsmallest(1, 'model_size_mb')['model_name'].iloc[0]}\n",
    "\n",
    "## Methodology\n",
    "\n",
    "All models were evaluated using:\n",
    "- Consistent train/validation splits (80/20)\n",
    "- Same preprocessing pipeline\n",
    "- Identical evaluation metrics\n",
    "- Hardware: {DEVICE}\n",
    "- Batch size: {BATCH_SIZE}\n",
    "\n",
    "Generated on: {time.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "    \n",
    "    with open(f\"{OUTPUT_DIR}/evaluation_report.md\", \"w\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "# --- Main Execution ---\n",
    "def main():\n",
    "    # Load dataset and class info\n",
    "    global full_dataset\n",
    "    full_dataset = datasets.ImageFolder(root=DATA_DIR)\n",
    "    class_names = list(full_dataset.class_to_idx.keys())\n",
    "    NUM_CLASSES = len(class_names)\n",
    "    \n",
    "    print(f\"Found {NUM_CLASSES} classes: {class_names}\")\n",
    "    \n",
    "    # Create validation split (same as training)\n",
    "    random.seed(42)\n",
    "    indices = list(range(len(full_dataset)))\n",
    "    random.shuffle(indices)\n",
    "    split_point = int(0.8 * len(indices))\n",
    "    val_indices = indices[split_point:]\n",
    "    \n",
    "    print(f\"Validation set: {len(val_indices)} samples\")\n",
    "    \n",
    "    # Get all available models\n",
    "    model_names = get_available_models()\n",
    "    print(f\"Found {len(model_names)} trained models: {model_names}\")\n",
    "    \n",
    "    if not model_names:\n",
    "        print(\"No trained models found! Please check the trained_models directory.\")\n",
    "        return\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Evaluate each model\n",
    "    for model_name in model_names:\n",
    "        try:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Processing: {model_name}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # --- NEW: Functionality to skip existing evaluations ---\n",
    "            json_output_path = f\"{OUTPUT_DIR}/metrics/{model_name}_metrics.json\"\n",
    "            if os.path.exists(json_output_path):\n",
    "                print(f\"Results for {model_name} already exist. Skipping evaluation, loading from file.\")\n",
    "                with open(json_output_path, 'r') as f:\n",
    "                    results = json.load(f)\n",
    "                all_results.append(results)\n",
    "                # Still generate the confusion matrix plot if it's missing\n",
    "                cm_path = f\"{OUTPUT_DIR}/confusion_matrices/{model_name}_confusion_matrix.png\"\n",
    "                if not os.path.exists(cm_path):\n",
    "                    plot_confusion_matrix(np.array(results[\"confusion_matrix\"]), \n",
    "                                          class_names, model_name)\n",
    "                continue # Move to the next model\n",
    "            # --- END NEW ---\n",
    "\n",
    "            # Extract image size from model name\n",
    "            img_size = extract_image_size_from_model_name(model_name)\n",
    "            print(f\"Using image size: {img_size}\")\n",
    "            \n",
    "            # Create transforms and dataset for this model\n",
    "            model_transforms = get_model_transforms(img_size)\n",
    "            model_dataset = datasets.ImageFolder(root=DATA_DIR, transform=model_transforms)\n",
    "            \n",
    "            # Create dataloader with validation split\n",
    "            val_sampler = SubsetRandomSampler(val_indices)\n",
    "            val_loader = DataLoader(model_dataset, batch_size=BATCH_SIZE, \n",
    "                                    sampler=val_sampler, num_workers=NUM_WORKERS, \n",
    "                                    pin_memory=True)\n",
    "            \n",
    "            # Load model\n",
    "            model = timm.create_model(model_name, pretrained=False, num_classes=NUM_CLASSES)\n",
    "            model_path = f\"{TRAINED_MODELS_DIR}/{model_name}_best.pth\"\n",
    "            \n",
    "            if os.path.exists(model_path):\n",
    "                model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "                model = model.to(DEVICE)\n",
    "                \n",
    "                # Evaluate model\n",
    "                results = evaluate_model_comprehensive(model, val_loader, model_name, img_size)\n",
    "                all_results.append(results)\n",
    "                \n",
    "                # Save individual results\n",
    "                with open(json_output_path, \"w\") as f: # MODIFIED: Use pre-defined path\n",
    "                    json.dump(results, f, indent=4)\n",
    "                \n",
    "                # Create confusion matrix\n",
    "                plot_confusion_matrix(np.array(results[\"confusion_matrix\"]), \n",
    "                                      class_names, model_name)\n",
    "                \n",
    "                # Print summary\n",
    "                print(f\"\\nResults Summary for {model_name}:\")\n",
    "                print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
    "                print(f\"  F1-Score: {results['f1_score']:.4f}\")\n",
    "                print(f\"  Model Size: {results['model_size_mb']:.2f} MB\")\n",
    "                print(f\"  Inference Time: {results['inference_time_ms']:.2f} ms\")\n",
    "                print(f\"  Throughput: {results['throughput_fps']:.1f} FPS\")\n",
    "                print(f\"  Efficiency Score: {results['efficiency_score']:.4f}\")\n",
    "                \n",
    "                # Clean up memory\n",
    "                del model\n",
    "                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "                \n",
    "            else:\n",
    "                print(f\"Model weights not found: {model_path}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {model_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_results:\n",
    "        print(\"No models were successfully evaluated!\")\n",
    "        return\n",
    "    \n",
    "    # Create comprehensive results DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Save complete results\n",
    "    results_df.to_csv(f\"{OUTPUT_DIR}/complete_results.csv\", index=False)\n",
    "    results_df.to_excel(f\"{OUTPUT_DIR}/complete_results.xlsx\", index=False)\n",
    "    \n",
    "    # Create visualizations\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    plot_model_comparison(results_df)\n",
    "    \n",
    "    # Create detailed report\n",
    "    print(\"Creating detailed report...\")\n",
    "    create_detailed_report(results_df, class_names)\n",
    "    \n",
    "    # Print final summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EVALUATION COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total models analyzed: {len(all_results)}\") # MODIFIED: Changed \"evaluated\" to \"analyzed\"\n",
    "    print(f\"Results saved to: {OUTPUT_DIR}/\")\n",
    "    print(f\"Best accuracy: {results_df['accuracy'].max():.4f} ({results_df.loc[results_df['accuracy'].idxmax(), 'model_name']})\")\n",
    "    print(f\"Most efficient: {results_df['efficiency_score'].max():.4f} ({results_df.loc[results_df['efficiency_score'].idxmax(), 'model_name']})\")\n",
    "    print(f\"Fastest inference: {results_df['inference_time_ms'].min():.2f}ms ({results_df.loc[results_df['inference_time_ms'].idxmin(), 'model_name']})\")\n",
    "    print(f\"Smallest model: {results_df['model_size_mb'].min():.2f}MB ({results_df.loc[results_df['model_size_mb'].idxmin(), 'model_name']})\")\n",
    "    \n",
    "    print(f\"\\nCheck the '{OUTPUT_DIR}' folder for:\")\n",
    "    print(\"- complete_results.csv/xlsx: All metrics in spreadsheet format\")\n",
    "    print(\"- evaluation_report.md: Detailed markdown report\")\n",
    "    print(\"- confusion_matrices/: Individual confusion matrices\")\n",
    "    print(\"- visualizations/: Comparison charts and plots\")\n",
    "    print(\"- metrics/: Individual model JSON results\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
